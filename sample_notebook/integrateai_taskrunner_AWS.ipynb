{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integrate.ai API Sample Notebook to run tasks with an AWS task runner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook that demonstrates creating taskbuilders and running tasks using an AWS task runner. \n",
    "For details about required setup and configuration for task runners, see [Using integrate.ai](https://documentation.integrateai.net/#using-integrate-ai)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Set environment variables (or replace inline) with your IAI credentials\n",
    "Generate and manage this token in the UI, in the Tokens page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.api import connect\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "IAI_TOKEN = os.environ.get(\"IAI_TOKEN\")\n",
    "\n",
    "client = connect(token=IAI_TOKEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your AWS variables\n",
    "\n",
    "**Important: The task runner expects your data to be in the bucket that was created when the task runner was provisioned.**\n",
    "\n",
    "This bucket name takes the form of: `s3://{aws_taskrunner_profile}-{aws_taskrunner_name}.integrate.ai`\n",
    "\n",
    "For example: `myworkspace-mytaskrunner.integrate.ai`\n",
    "\n",
    "You can download sample data from the integrate.ai sample bucket:\n",
    "\n",
    "For HFL and EDA: [https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/synthetic.zip](https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/synthetic.zip)\n",
    "\n",
    "For PRL and VFL: [https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/vfl.zip](https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/synthetic.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_taskrunner_profile = \"<workspace>\" # This is your workspace name\n",
    "aws_taskrunner_name = \"<taskrunner name>\" # Task runner name - must match what was supplied in UI to create task runner\n",
    "\n",
    "base_aws_bucket = f'{aws_taskrunner_profile}-{aws_taskrunner_name}.integrate.ai'\n",
    "\n",
    "# Example datapaths. Make sure that the data you want to work with exists in the base_aws_bucket for your task runner.\n",
    "# HFL datapaths\n",
    "train_path1 = f's3://{base_aws_bucket}/synthetic/train_silo0.parquet'\n",
    "test_path1 = f's3://{base_aws_bucket}/synthetic/test.parquet'\n",
    "train_path2 = f's3://{base_aws_bucket}/synthetic/train_silo1.parquet'\n",
    "test_path2 = f's3://{base_aws_bucket}/synthetic/test.parquet'\n",
    "\n",
    "#EDA/PRL/VFL datapaths\n",
    "active_train_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/active_train.parquet'\n",
    "active_test_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/active_test.parquet'\n",
    "passive_train_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/passive_train.parquet'\n",
    "passive_test_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/passive_test.parquet'\n",
    "\n",
    "#Where to store the trained model\n",
    "aws_storage_path = f's3://{base_aws_bucket}/model'\n",
    "\n",
    "#Where to store VFL predictions - must be full path and file name\n",
    "vfl_predict_active_storage_path = f's3://{base_aws_bucket}/vfl_predict/active_predictions.csv'\n",
    "vfl_predict_passive_storage_path = f's3://{base_aws_bucket}/vfl_predict/passive_predictions.csv'\n",
    "\n",
    "base_aws_bucket #Prints the base_aws_bucket name for reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the taskbuilder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import integrate_ai_sdk\n",
    "from integrate_ai_sdk.taskgroup.taskbuilder.integrate_ai import IntegrateAiTaskBuilder\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup\n",
    "from integrate_ai_sdk.taskgroup.taskbuilder import taskrunner_context\n",
    "from integrate_ai_sdk.taskgroup.taskbuilder.integrate_ai import IntegrateAiTaskBuilder\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup\n",
    "\n",
    "iai_tb_aws = IntegrateAiTaskBuilder(client=client,\n",
    "   task_runner_id=aws_taskrunner_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Perform EDA in Individual Mode\n",
    "\n",
    "This example task demonstrates how to run an exploratory data analysis (EDA) session in Individual mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset configuration\n",
    "\n",
    "dataset_config = {\"dataset_one\": [], \"dataset_two\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start the EDA session \n",
    "\n",
    "eda_session = client.create_eda_session(\n",
    "    name=\"Testing notebook - EDA\",\n",
    "    description=\"I am testing EDA session creation with a task runner through a notebook\",\n",
    "    data_config=dataset_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "eda_session.id  #Prints the EDA session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "eda_task_group_context = (\n",
    "        SessionTaskGroup(eda_session) \\\n",
    "        .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "        .add_task(iai_tb_aws.eda(dataset_name=\"dataset_one\", dataset_path=train_path1))\\\n",
    "        .add_task(iai_tb_aws.eda(dataset_name=\"dataset_two\", dataset_path=train_path2))\\\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the task group status\n",
    "\n",
    "for i in eda_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "eda_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "eda_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: EDA session complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view eda session results"
    ]
   },
   "outputs": [],
   "source": [
    "# Session complete, retrieve the results \n",
    "\n",
    "results = eda_session.results()[\"dataset_one\", \"dataset_two\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get shape of results"
    ]
   },
   "outputs": [],
   "source": [
    "results.mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get info from col"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_one = eda_session.results()[\"dataset_one\"]\n",
    "dataset_one_count = dataset_one[\"x0\"].count()\n",
    "dataset_one[\"x0\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Create an HFL FFNet Training Session\n",
    "\n",
    "The documentation for [creating a session](https://documentation.integrateai.net/#tutorial-ffnet-model-training-with-a-sample-local-dataset-iai_ffnet) gives additional context into the parameters that are used during training session creation.<br />\n",
    "For this session we are going to be using two training clients and two rounds. \n",
    "\n",
    "You can find the model config and data schema details in the [integrate.ai end user tutorial](https://documentation.integrateai.net/#understanding-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assign schemas"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config = {\n",
    "    \"experiment_name\": \"test_synthetic_tabular\",\n",
    "    \"experiment_description\": \"test_synthetic_tabular\",\n",
    "    \"strategy\": {\"name\": \"FedAvg\", \"params\": {}},\n",
    "    \"model\": {\"params\": {\"input_size\": 15, \"hidden_layer_sizes\": [6, 6, 6], \"output_size\": 2}},\n",
    "    \"balance_train_datasets\": False,\n",
    "    \"ml_task\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"params\": {\n",
    "            \"loss_weights\": None,\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.2, \"momentum\": 0.0}},\n",
    "    \"differential_privacy_params\": {\"epsilon\": 4, \"max_grad_norm\": 7},\n",
    "    \"save_best_model\": {\n",
    "        \"metric\": \"loss\",  # to disable this and save model from the last round, set to None\n",
    "        \"mode\": \"min\",\n",
    "    },\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_schema = {\n",
    "    \"predictors\": [\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\", \"x11\", \"x12\", \"x13\", \"x14\"],\n",
    "    \"target\": \"y\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start the training session\n",
    "\n",
    "training_session = client.create_fl_session(\n",
    "    name=\"Testing notebook\",\n",
    "    description=\"I am testing session creation with a task runner through a notebook\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_ffnet\",\n",
    "    model_config=model_config,\n",
    "    data_config=data_schema,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "training_session.id # Prints the training session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start batch clients"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "task_group = (\n",
    "    SessionTaskGroup(training_session)\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path1, test_path=test_path1, use_gpu=False))\\\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path2, test_path=test_path2, use_gpu=False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_group_context = task_group.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the submitted tasks\n",
    "\n",
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: HFL Session Complete!\n",
    "Now you can view the training metrics and start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# Retrieve the session metrics\n",
    "\n",
    "training_session.metrics().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the session metrics\n",
    "\n",
    "fig = training_session.metrics().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Create a PRL Session for linking two or more datasets\n",
    "\n",
    "To create a PRL session, specify a `dataset_config` dictionary indicating the client names and columns to use as identifiers to link the datasets to each other. The number of expected clients will be inferred as the number of items in dataset_config (i.e., two). These client names are referenced for the compute on the PRL session and for any sessions that use the PRL session downstream.\n",
    "\n",
    "For this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create prl data config"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify PRL dataset configuration \n",
    "\n",
    "prl_data_config = {\n",
    "    \"clients\": {\n",
    "        \"active_client\": {\"id_columns\": [\"id\"]},\n",
    "        \"passive_client\": {\"id_columns\": [\"id\"]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create prl session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start PRL session\n",
    "\n",
    "prl_session = client.create_prl_session(\n",
    "    name=\"Testing notebook - PRL\",\n",
    "    description=\"I am testing PRL session creation with a task runner through a notebook\",\n",
    "    data_config=prl_data_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "prl_session.id #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start prl training batch clients"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "task_group = (SessionTaskGroup(prl_session)\\\n",
    ".add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    ".add_task(iai_tb_aws.prl(train_path=active_train_path, test_path=active_test_path, client_name=\"active_client\"))\\\n",
    ".add_task(iai_tb_aws.prl(train_path=passive_train_path, test_path=passive_test_path, client_name=\"passive_client\"))\n",
    ")\n",
    "\n",
    "task_group_context = task_group.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the status of the task group\n",
    "\n",
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: PRL Session Complete!\n",
    "Now you can view the overlap stats for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view prl session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# View PRL session metrics\n",
    "\n",
    "metrics = prl_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a: Create a VFL Training Session using the PRL session from Task 3\n",
    "\n",
    "To create a VFL train session, specify the `prl_session_id` indicating the session you just ran to link the datasets together. The `vfl_mode` needs to be set to `train`.\n",
    "\n",
    "For more information about vertical federated learning with a SplitNN model strategy, see [VFL SplitNN Model Trianing](https://documentation.integrateai.net/#vfl-splitnn-model-training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "set vfl data config"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config = {\n",
    "    \"strategy\": {\"name\": \"SplitNN\", \"params\": {}},\n",
    "    \"model\": {\n",
    "        \"feature_models\": {\n",
    "            \"passive_client\": {\"params\": {\"input_size\": 7, \"hidden_layer_sizes\": [6], \"output_size\": 5}},\n",
    "            \"active_client\": {\"params\": {\"input_size\": 8, \"hidden_layer_sizes\": [6], \"output_size\": 5}},\n",
    "        },\n",
    "        \"label_model\": {\"params\": {\"hidden_layer_sizes\": [5], \"output_size\": 2}},\n",
    "    },\n",
    "    \"ml_task\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"params\": {\n",
    "            \"loss_weights\": None,\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.2, \"momentum\": 0.0}},\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "        \"passive_client\": {\n",
    "            \"label_client\": False,\n",
    "            \"predictors\": [\"x1\", \"x3\", \"x5\", \"x7\", \"x9\", \"x11\", \"x13\"],\n",
    "            \"target\": None,\n",
    "        },\n",
    "        \"active_client\": {\n",
    "            \"label_client\": True,\n",
    "            \"predictors\": [\"x0\", \"x2\", \"x4\", \"x6\", \"x8\", \"x10\", \"x12\", \"x14\"],\n",
    "            \"target\": \"y\",\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create training session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a VFL training session\n",
    "\n",
    "vfl_train_session = client.create_vfl_session(\n",
    "    name=\"Testing notebook - VFL Train\",\n",
    "    description=\"I am testing VFL Train session creation with a task runner through a notebook\",\n",
    "    prl_session_id=prl_session.id,\n",
    "    vfl_mode='train',\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_ffnet\",\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "vfl_train_session.id    #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start training task"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "vfl_task_group_context = (SessionTaskGroup(vfl_train_session)\\\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.vfl_train(train_path=active_train_path, \n",
    "                                    test_path=active_test_path, \n",
    "                                    batch_size=1024,  \n",
    "                                    client_name=\"active_client\", \n",
    "                                    storage_path = f\"{aws_storage_path}/vfl/{vfl_train_session.id}\"))\\\n",
    "    .add_task(iai_tb_aws.vfl_train(train_path=passive_train_path, \n",
    "                                    test_path=passive_test_path, \n",
    "                                    batch_size=1024, \n",
    "                                    client_name=\"passive_client\", \n",
    "                                    storage_path = f\"{aws_storage_path}/vfl/{vfl_train_session.id}\"))\\\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check training task session id"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the status of the tasks\n",
    "\n",
    "for i in vfl_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "vfl_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "vfl_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: VFL Session Complete!\n",
    "Now you can view the VFL training metrics and start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view training metrics"
    ]
   },
   "outputs": [],
   "source": [
    "metrics = vfl_train_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vfl_train_session.metrics().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b: Make a Prediction on the trained VFL model\n",
    "\n",
    "To create a VFL predict session, specify the `prl_session_id` indicating the session you ran to link the datasets together. You also need the `training_id` of the VFL train session that was run using the same `prl_session_id`. \n",
    "\n",
    "The `vfl_mode` must be set to `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create predict session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a VFL predict session\n",
    "\n",
    "vfl_predict_session = client.create_vfl_session(\n",
    "    name=\"Testing notebook - VFL Predict\",\n",
    "    description=\"I am testing VFL Predict session creation with an AWS task runner through a notebook\",\n",
    "    prl_session_id=prl_session.id,\n",
    "    training_session_id=vfl_train_session.id,\n",
    "    vfl_mode=\"predict\",\n",
    "    data_config=data_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "vfl_predict_session.id  # Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start prediction task"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "vfl_predict_task_group_context = (SessionTaskGroup(vfl_predict_session)\\\n",
    ".add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    ".add_task(iai_tb_aws.vfl_predict(\n",
    "        client_name=\"active_client\", \n",
    "        dataset_path=active_test_path, \n",
    "        raw_output=True,\n",
    "        batch_size=1024, \n",
    "        storage_path=vfl_predict_active_storage_path))\\\n",
    ".add_task(iai_tb_aws.vfl_predict(\n",
    "        client_name=\"passive_client\",\n",
    "        dataset_path=passive_test_path,\n",
    "        batch_size=1024,\n",
    "        raw_output=True,\n",
    "        storage_path=vfl_predict_passive_storage_path))\\\n",
    ".start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the tasks\n",
    "\n",
    "for i in vfl_predict_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "vfl_predict_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "vfl_predict_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: VFL Predict Session Complete!\n",
    "\n",
    "Now you can view the VFL predictions and evaluate the performance as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view predict session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# Retrieve the metrics\n",
    "\n",
    "metrics = vfl_predict_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presigned_result_urls = vfl_predict_session.prediction_result()\n",
    "\n",
    "print(vfl_predict_active_storage_path)\n",
    "df_pred = pd.read_csv(presigned_result_urls.get(vfl_predict_active_storage_path))\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Create a linear inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config_logit = {\n",
    "    \"strategy\": {\"name\": \"LogitRegInference\", \"params\": {}},\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_config_logit = {\n",
    "    \"target\": \"y\",\n",
    "    \"shared_predictors\": [\"x1\", \"x2\"],\n",
    "    \"chunked_predictors\": [\"x0\", \"x3\", \"x10\", \"x11\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a linear inference session \n",
    "\n",
    "training_session_logit = client.create_fl_session(\n",
    "    name=\"Testing linear inference session\",\n",
    "    description=\"I am testing linear inference session creation using a task runner through a notebook\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=5,\n",
    "    package_name=\"iai_linear_inference\",\n",
    "    model_config=model_config_logit,\n",
    "    data_config=data_config_logit,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "training_session_logit.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a task group\n",
    "\n",
    "task_group_context = (\n",
    "    SessionTaskGroup(training_session_logit)\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path1, test_path=test_path1))\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path2, test_path=test_path2)).start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Session Complete!\n",
    "Now we can view the training metrics and model details such as the model coefficients and p-values. Note that since there are a bundle of models being trained, the metrics below are the average values of all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_session_logit.metrics().as_dict()\n",
    "training_session_logit.metrics().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained models are accessible from the completed session\n",
    "\n",
    "The `LinearInferenceModel` object can be retrieved using the model's `as_pytorch` method. And the relevant information such as p-values can be accessed directly from the model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit = training_session_logit.model().as_pytorch()\n",
    "pv = model_logit.p_values()\n",
    "pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.summary` method fetches the coefficient, standard error and p-value of the model corresponding to the specified predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_x0 = model_logit.summary(\"x0\")\n",
    "summary_x0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to make predictions with the resulting bundle of models, when the data is loaded by the `ChunkedTabularDataset` from the `iai_linear_inference` package. For an example of this, see the `integrateai_linear_inference.ipynb` notebook in the [sample repo](https://github.com/integrateai/integrateai-samples/tree/main/sample_notebook)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f8223dd0c77fd300f5027a07540ab82ef7f159d3d8a00663aa8a0c2ca691ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
