{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integrate.ai API Sample Notebook to run tasks with an AWS task runner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook that demonstrates creating taskbuilders and running tasks using an AWS task runner. \n",
    "For details about required setup and configuration for task runners, see [Using integrate.ai](https://documentation.integrateai.net/#using-integrate-ai)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Set environment variables (or replace inline) with your IAI credentials\n",
    "Generate and manage this token in the UI, in the Tokens page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.api import connect\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "IAI_TOKEN = \"\"\n",
    "client = connect(token=IAI_TOKEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Download the sample data\n",
    "\n",
    "You can download sample data from the integrate.ai sample bucket:\n",
    "\n",
    "For HFL and EDA: [https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/synthetic.zip](https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/synthetic.zip)\n",
    "\n",
    "For PRL and VFL: [https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/vfl.zip](https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/integrate_ai_examples/vfl.zip)\n",
    "\n",
    "### Create a task runner in your workspace\n",
    "\n",
    "For instructions for how to create an AWS task runner, [see the documentation](https://documentation.integrateai.net/#create-an-aws-task-runner). \n",
    "\n",
    "### Upload the sample data to the S3 bucket created for your task runner\n",
    "\n",
    "**Important: By default the task runner expects your data to be in the bucket that was created when the task runner was provisioned.**\n",
    "\n",
    "This bucket name takes the form of: `s3://{aws_traskrunner_profile}-{aws_taskrunner_name}.integrate.ai`\n",
    "\n",
    "For example: `myworkspace-mytaskrunner.integrate.ai`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_taskrunner_profile = \"<myworkspace>\" # This is your workspace name\n",
    "aws_taskrunner_name = \"<mytaskrunner>\" # Task runner name - must match what was supplied in UI to create task runner\n",
    "\n",
    "base_aws_bucket = f'{aws_taskrunner_profile}-{aws_taskrunner_name}.integrate.ai'\n",
    "\n",
    "#Where to store the trained model\n",
    "aws_storage_path = f's3://{base_aws_bucket}/model'\n",
    "\n",
    "base_aws_bucket #Prints the base_aws_bucket name for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the sample datasets\n",
    "\n",
    "For instructions for how to register datasets, [see the documentation](https://documentation.integrateai.net/#register-a-dataset-aws). \n",
    "\n",
    "**In your workspace, register the sample datasets with your task runner with the following names. Replace {base_aws_bucket} with the bucket name for your environment.**\n",
    "\n",
    "train_one = s3://{base_aws_bucket}/synthetic/train_silo0.parquet\n",
    "\n",
    "train_two = s3://{base_aws_bucket}/synthetic/train_silo1.parquet\n",
    "\n",
    "test_parquet = s3://{base_aws_bucket}/synthetic/test.parquet\n",
    "\n",
    "active_train = s3://{base_aws_bucket}/vfl/active_train.parquet\n",
    "\n",
    "passive_train = s3://{base_aws_bucket}/vfl/passive_train.parquet\n",
    "\n",
    "active_test = s3://{base_aws_bucket}/vfl/active_test.parquet\n",
    "\n",
    "passive_test = s3://{base_aws_bucket}/vfl/passive_test.parquet\n",
    "\n",
    "\n",
    "**Note:** If you use other datasets or change the names, you **must** update the dataset names in the code example below to run a session succesfully. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the taskbuilder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.taskgroup.taskbuilder.integrate_ai import IntegrateAiTaskBuilder\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup\n",
    "\n",
    "iai_tb_aws = IntegrateAiTaskBuilder(client=client,\n",
    "   task_runner_id=aws_taskrunner_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Perform EDA in Individual Mode\n",
    "\n",
    "This example task demonstrates how to run an exploratory data analysis (EDA) session in Individual mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset configuration\n",
    "# Note that this example uses registered dataset names \n",
    "\n",
    "dataset_config = {\"train_one\": [], \"train_two\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start the EDA session \n",
    "\n",
    "eda_session = client.create_eda_session(\n",
    "    name=\"Testing notebook - EDA\",\n",
    "    description=\"I am testing EDA session creation with a task runner through a notebook\",\n",
    "    data_config=dataset_config,\n",
    ").start()\n",
    "\n",
    "eda_session.id  #Prints the EDA session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task group with one task for each of the clients joining the session\n",
    "\n",
    "eda_task_group_context = (\n",
    "        SessionTaskGroup(eda_session) \\\n",
    "        .add_task(iai_tb_aws.eda(dataset_name=\"train_one\"))\\\n",
    "        .add_task(iai_tb_aws.eda(dataset_name=\"train_two\"))\\\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the task group status\n",
    "\n",
    "for i in eda_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "eda_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "eda_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: EDA session complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session complete, retrieve the results \n",
    "\n",
    "results = eda_session.results()[\"train_one\"]\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_one = eda_session.results()[\"train_one\"]\n",
    "dataset_one_count = dataset_one[\"x0\"].count()\n",
    "dataset_one[\"x0\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Create a PRL Session for linking two or more datasets\n",
    "\n",
    "To create a PRL session, specify a `dataset_config` dictionary indicating the client names and columns to use as identifiers to link the datasets to each other. The number of expected clients will be inferred as the number of items in dataset_config (i.e., two). These client names are referenced for the compute on the PRL session and for any sessions that use the PRL session downstream.\n",
    "\n",
    "For this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients active_client and passive_client respectively. Their datasets will be linked by the \"id\" column in any provided datasets.\n",
    "\n",
    "Detailed information about PRL is available in the [documentation](https://documentation.integrateai.net/#private-record-linkage-prl-sessions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify PRL dataset configuration \n",
    "\n",
    "prl_data_config = {\n",
    "  \"clients\": {\n",
    "    \"active_client\": {\n",
    "      \"id_columns\": [\n",
    "        \"id\"\n",
    "      ],\n",
    "    },\n",
    "    \"passive_client\": {\n",
    "      \"id_columns\": [\n",
    "        \"id\"\n",
    "      ],\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start PRL session\n",
    "\n",
    "prl_session = client.create_prl_session(\n",
    "    name=\"Testing notebook - PRL\",\n",
    "    description=\"I am testing PRL session creation with a task runner through a notebook\",\n",
    "    data_config=prl_data_config\n",
    ").start()\n",
    "\n",
    "prl_session.id #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task group with one task for each of the clients joining the session\n",
    "\n",
    "task_group = (SessionTaskGroup(prl_session)\\\n",
    "    .add_task(iai_tb_aws.prl(train_dataset_name=\"active_train\", test_dataset_name=\"active_test\", client_name=\"active_client\"))\\\n",
    "    .add_task(iai_tb_aws.prl(train_dataset_name=\"passive_train\", test_dataset_name=\"passive_test\", client_name=\"passive_client\"))\n",
    ")\n",
    "\n",
    "task_group_context = task_group.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the status of the task group\n",
    "\n",
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: PRL Session Complete!\n",
    "Now you can view the overlap stats for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View PRL session metrics\n",
    "\n",
    "metrics = prl_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create a VFL Training Session using the PRL session from Task 2\n",
    "\n",
    "To create a VFL train session, specify the `prl_session_id` indicating the session you just ran to link the datasets together. The `vfl_mode` needs to be set to `train`.\n",
    "\n",
    "For more information about vertical federated learning with a SplitNN model strategy, see the [documentation](https://documentation.integrateai.net/#vfl-splitnn-model-training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config = {\n",
    "    \"strategy\": {\"name\": \"SplitNN\", \"params\": {}},\n",
    "    \"model\": {\n",
    "        \"feature_models\": {\n",
    "            \"passive_client\": {\"params\": {\"input_size\": 7, \"hidden_layer_sizes\": [6], \"output_size\": 5}},\n",
    "            \"active_client\": {\"params\": {\"input_size\": 8, \"hidden_layer_sizes\": [6], \"output_size\": 5}},\n",
    "        },\n",
    "        \"label_model\": {\"params\": {\"hidden_layer_sizes\": [5], \"output_size\": 2}},\n",
    "    },\n",
    "    \"ml_task\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"params\": {\n",
    "            \"loss_weights\": None,\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.2, \"momentum\": 0.0}},\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "        \"passive_client\": {\n",
    "            \"label_client\": False,\n",
    "            \"predictors\": [\"x1\", \"x3\", \"x5\", \"x7\", \"x9\", \"x11\", \"x13\"],\n",
    "            \"target\": None,\n",
    "        },\n",
    "        \"active_client\": {\n",
    "            \"label_client\": True,\n",
    "            \"predictors\": [\"x0\", \"x2\", \"x4\", \"x6\", \"x8\", \"x10\", \"x12\", \"x14\"],\n",
    "            \"target\": \"y\",\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a VFL training session\n",
    "\n",
    "vfl_train_session = client.create_vfl_session(\n",
    "    name=\"Testing notebook - VFL Train\",\n",
    "    description=\"I am testing VFL Train session creation with a task runner through a notebook\",\n",
    "    prl_session_id=prl_session.id,\n",
    "    vfl_mode='train',\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_ffnet\",       # this session is using a SplitNN method\n",
    "    data_config=data_config,\n",
    "    model_config=model_config\n",
    ").start()\n",
    "\n",
    "vfl_train_session.id    #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for each of the clients joining the session\n",
    "# This example uses registered dataset names.\n",
    "\n",
    "storage_path = f\"{aws_storage_path}/vfl/{vfl_train_session.id}\"\n",
    "\n",
    "vfl_task_group_context = (SessionTaskGroup(vfl_train_session)\\\n",
    "    .add_task(iai_tb_aws.vfl_train(train_dataset_name=\"active_train\", test_dataset_name=\"active_test\", \n",
    "                                    batch_size=1024,  \n",
    "                                    client_name=\"active_client\", \n",
    "                                    storage_path = storage_path))\\\n",
    "    .add_task(iai_tb_aws.vfl_train(train_dataset_name=\"passive_train\", test_dataset_name=\"passive_test\", \n",
    "                                    batch_size=1024, \n",
    "                                    client_name=\"passive_client\", \n",
    "                                    storage_path = storage_path))\\\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the tasks\n",
    "\n",
    "for i in vfl_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "vfl_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "vfl_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: VFL Session Complete!\n",
    "Now you can view the VFL training metrics and start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = vfl_train_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vfl_train_session.metrics().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Make a Prediction on the trained VFL model\n",
    "\n",
    "To create a VFL predict session, specify the `prl_session_id` indicating the session you ran to link the datasets together. You also need the `training_id` of the VFL train session that was run using the same `prl_session_id`. \n",
    "\n",
    "The `vfl_mode` must be set to `predict`.\n",
    "\n",
    "For more information, see the [documentation](https://documentation.integrateai.net/#vfl-prediction-session-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create predict session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a VFL predict session\n",
    "\n",
    "vfl_predict_session = client.create_vfl_session(\n",
    "    name=\"Testing notebook - VFL Predict\",\n",
    "    description=\"I am testing VFL Predict session creation with an AWS task runner through a notebook\",\n",
    "    prl_session_id=prl_session.id,\n",
    "    training_session_id=vfl_train_session.id,\n",
    "    vfl_mode=\"predict\",\n",
    "    data_config=data_config\n",
    ").start()\n",
    "\n",
    "vfl_predict_session.id  # Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start prediction task"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for the server, and one for each of the clients joining the session\n",
    "# This example uses registered dataset names\n",
    "\n",
    "storage_path = f\"{vfl_predict_active_storage_path}_{vfl_predict_session.id}.csv\"\n",
    "\n",
    "vfl_predict_task_group_context = (SessionTaskGroup(vfl_predict_session)\\\n",
    ".add_task(iai_tb_aws.vfl_predict(\n",
    "        client_name=\"active_client\", \n",
    "        dataset_name=\"active_test\",\n",
    "        raw_output=True,\n",
    "        batch_size=1024, \n",
    "        storage_path = storage_path))\\\n",
    ".add_task(iai_tb_aws.vfl_predict(\n",
    "        client_name=\"passive_client\",\n",
    "        dataset_name=\"passive_test\",\n",
    "        batch_size=1024,\n",
    "        raw_output=True,\n",
    "        storage_path = storage_path))\\\n",
    ".start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the tasks\n",
    "\n",
    "for i in vfl_predict_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "vfl_predict_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "vfl_predict_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VFL Predict Session Complete!\n",
    "\n",
    "Now you can view the VFL predictions and evaluate the performance as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view predict session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# Retrieve the metrics\n",
    "\n",
    "metrics = vfl_predict_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presigned_result_urls = vfl_predict_session.prediction_result()\n",
    "\n",
    "print(vfl_predict_active_storage_path)\n",
    "df_pred = pd.read_csv(presigned_result_urls.get(storage_path))\n",
    "\n",
    "df_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f8223dd0c77fd300f5027a07540ab82ef7f159d3d8a00663aa8a0c2ca691ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
