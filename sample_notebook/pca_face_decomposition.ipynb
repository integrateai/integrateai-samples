{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated PCA for Face Images Decomposition\n",
    "\n",
    "Principal component analysis (PCA) is a dimesion reduction technique. It learns an **orthogonal linear transformation**, which transforms the original data into a new coordinate system, such that most of the variation can be explained by a few dimensions (known as the principal component directions). PCA helps uncover patterns in the data by representing it in a lower-dimension latent space, thus making it an important technique for exploratory analysis and data visualization. It is also very helpful in reducing the number of features for subsequent predictive models, enabling more efficient and robust modelling of high-dimensional data.\n",
    "\n",
    "In this example, we apply the federated version of PCA to decompose face images into a set of basis images, called the \"eigenfaces\", which can then be used to build face recognition models in downstream tasks. We consider the [Olivetti dataset](https://cam-orl.co.uk/facedatabase.html) consisting of 400 face images taken between April 1992 and April 1994 at AT&T Laboratories Cambridge. The original images are of size 64x64 and on greyscale with 256 levels. The pixels have been centered similar to this [example](https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#dataset-preparation). Ten percent (40) of the images are reserved for evaluation, while the rest are equally split into 2 sets (180 per set) representing local silos under a horizontal federated learning setting. Next we will show step-by-step how to perform federated PCA with the Integrate.ai SDK.\n",
    "\n",
    "\n",
    "## Setup\n",
    "1. Download the data from this [link](https://s3.ca-central-1.amazonaws.com/public.s3.integrate.ai/fl-demo/olivetti_faces.zip) and unzip it. There are 3 files under the directory `olivetti_faces` corresponding to the two siloed data and the evaulation data.\n",
    "2. Use the `integrate-ai` CLI tool to install the latest SDK and pull the client docker image. Some useful commands:\n",
    "    ```bash\n",
    "    pip install integrate-ai\n",
    "    iai --help\n",
    "    iai sdk install\n",
    "    iai client pull\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authorize and initialize the SDK client\n",
    "import os\n",
    "from integrate_ai_sdk.api import connect\n",
    "\n",
    "\n",
    "IAI_TOKEN = \"\"\n",
    "client = connect(token=IAI_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data directory where the olivetti_faces data is saved\n",
    "# Change this to a local directory on your machine\n",
    "DATA_DIR = \"~Downloads/olivetti_faces\"\n",
    "train_path1 = f\"{DATA_DIR}/client_1.parquet\"\n",
    "train_path2 = f\"{DATA_DIR}/client_2.parquet\"\n",
    "test_path = f\"{DATA_DIR}/test.parquet\"\n",
    "\n",
    "# Specify path where the model artifacts will be saved\n",
    "# Change this to a local directory on your machine\n",
    "storage_path = \"/tmp/iai-test-docker-taskbuilder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a quick look at the evaluation data to understand how it is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=3, image_shape=(64, 64), cmap=plt.cm.gray):\n",
    "    n_row = ceil(images.shape[0] / n_col)\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=n_row,\n",
    "        ncols=n_col,\n",
    "        figsize=(2.0 * n_col, 2.3 * n_row),\n",
    "        facecolor=\"white\",\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n",
    "    fig.set_edgecolor(\"black\")\n",
    "    fig.suptitle(title, size=16)\n",
    "    for ax, vec in zip(axs.flat, images):\n",
    "        vmax = max(vec.max(), -vec.min())\n",
    "        im = ax.imshow(\n",
    "            vec.reshape(image_shape),\n",
    "            cmap=cmap,\n",
    "            interpolation=\"nearest\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_faces = pd.read_parquet(test_path).values\n",
    "print(test_faces.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, there are 40 images in the evaluation set, each flattened as a 4096-dimensional vector (64x64). We can reshape them back into 2D arrays. The following cell shows the first 6 images in the dataset. Since human faces follow some consistent patterns, the pixels of these images are correlated. Intuitively, we do not need the full 4096 dimensions to characterize them. This motivates the application of PCA to find a low-dimension representation of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(\"Faces from dataset\", test_faces[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the follwoing model and data configuration for our federated PCA session. We consider the first 16 (`n_components`) PC directions to represent the data, reduced from the original 4096 dimensions. `whiten` can be set to `True` if the PCs will be used for downstream face recognition tasks. In data config, we set `predictors` to `[]` such that all 4096 input dimensions are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"strategy\": {\"name\": \"FedPCA\", \"params\": {}},\n",
    "    \"model\": {\n",
    "        \"params\": {\n",
    "            \"n_components\": 16,\n",
    "            \"whiten\": True\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "    \"predictors\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an FL session with the `iai_pca` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_session = client.create_fl_session(\n",
    "    name=\"pca_face_image_decomposition\",\n",
    "    description=\"apply PCA to decompose face images\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_pca\",\n",
    "    model_config=model_config,\n",
    "    data_config=data_config,\n",
    ").start()\n",
    "\n",
    "pca_session.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a task group for the session where the two clients (local silos) are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.taskgroup.taskbuilder import local\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup\n",
    "\n",
    "tb = local.local_docker(\n",
    "    client,\n",
    "    docker_login=False,\n",
    ")\n",
    "\n",
    "task_group_context = (\n",
    "    SessionTaskGroup(pca_session)\n",
    "    .add_task(tb.hfl(train_path=train_path1, test_path=test_path, client_name=\"client1\"))\\\n",
    "    .add_task(tb.hfl(train_path=train_path2, test_path=test_path, client_name=\"client2\"))\\\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the session to complete\n",
    "task_group_context.wait(60 * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can download the learned artifact as a PyTorch model object. The first 16 PC directions are available from the `components` attribute. Similar to the original images, we can reshape the PC directions into 2D arrays and visualize them as images, which are known as the \"eigenfaces\". The first 3 eigenfaces are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformation = pca_session.model().as_pytorch()\n",
    "\n",
    "print(f\"We get {pca_transformation.components.shape[0]} PC directions of length {pca_transformation.components.shape[1]}.\")\n",
    "\n",
    "plot_gallery(\"Eigenfaces\", pca_transformation.components[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of variance explained by each PC is stored in `explained_variance_ratio` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_proportion = pca_transformation.explained_variance_ratio.sum().item() * 100\n",
    "print(f\"A total of {total_proportion:.2f}% variance in the original data can be explained by the first 16 principal components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the model object to transform face images from the original feature space (4096 dimensions) to the new principal component space (16 dimensions). And downstream predictive models for face recognition can be built within this much lower dimension space, which is beyond the scope of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# transform the images in the evaluation set\n",
    "test_faces_transformed = pca_transformation(torch.tensor(test_faces))\n",
    "print(test_faces_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the transformed data do preserve most of the information in the original data, we can reconstruct the original images by multiplying the transformed data by the `components` array. This is possible because the PCA projection is orthogonal.\n",
    "\n",
    "The following cell shows the first 6 reconstructed images in the evaluation set. As we can observe, although a lot of details are lost, the reconstructed images still retain a fairly good amount of information about the raw faces. Note that this is achieved with only 16 dimensions, which is less than `0.5%` of the original data dimension. For this reason, PCA is also sometimes used for data compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_faces_recovered = test_faces_transformed @ pca_transformation.components\n",
    "# visualize the reconstructed faces\n",
    "plot_gallery(\"Faces reconstructed from eigenfaces\", test_faces_recovered[:6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iai_local_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
