{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integrate.ai API Sample Notebook to run tasks with an AWS task runner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example notebook that demonstrates creating taskbuilders and running tasks using an AWS task runner. \n",
    "For details about required setup and configuration for task runners, see [Using integrate.ai](https://documentation.integrateai.net/#using-integrate-ai)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Set environment variables (or replace inline) with your IAI credentials\n",
    "Generate and manage this token in the UI, in the Tokens page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.api import connect\n",
    "import os\n",
    "\n",
    "IAI_TOKEN = \"\n",
    "client = connect(token=IAI_TOKEN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your AWS variables\n",
    "\n",
    "**Important: The task runner expects your data to be in the bucket that was created when the task runner was provisioned.**\n",
    "\n",
    "This bucket name takes the form of: `s3://{aws_taskrunner_profile}-{aws_taskrunner_name}.integrate.ai`\n",
    "\n",
    "For example: `myworkspace-mytaskrunner.integrate.ai`\n",
    "\n",
    "You can download sample data from the integrate.ai sample bucket [s3://public.s3.integrate.ai/integrate_ai_examples/vfl/](s3://public.s3.integrate.ai/integrate_ai_examples/vfl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<myworkspace>-<mytaskrunner>.integrate.ai'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws_taskrunner_profile = \"<myworkspace>\" # This is your workspace name\n",
    "aws_taskrunner_name = \"<mytaskrunner>\" # Task runner name - must match what was supplied in UI to create task runner\n",
    "\n",
    "base_aws_bucket = f'{aws_taskrunner_profile}-{aws_taskrunner_name}.integrate.ai'\n",
    "\n",
    "# Example datapaths. Make sure that the data you want to work with exists in the base_aws_bucket for your task runner.\n",
    "# HFL datapaths\n",
    "train_path1 = f's3://{base_aws_bucket}/synthetic/train_silo0.parquet'\n",
    "test_path1 = f's3://{base_aws_bucket}/synthetic/test.parquet'\n",
    "train_path2 = f's3://{base_aws_bucket}/synthetic/train_silo1.parquet'\n",
    "test_path2 = f's3://{base_aws_bucket}/synthetic/test.parquet'\n",
    "\n",
    "#EDA/PRL/VFL datapaths\n",
    "active_train_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/active_train.parquet'\n",
    "active_test_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/active_test.parquet'\n",
    "passive_train_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/passive_train.parquet'\n",
    "passive_test_path = f's3://{base_aws_bucket}/synthetic_prl_vfl/passive_test.parquet'\n",
    "\n",
    "#Where to store the trained model\n",
    "aws_storage_path = f's3://{base_aws_bucket}/model'\n",
    "\n",
    "#Where to store VFL predictions - must be full path and file name\n",
    "vfl_predict_active_storage_path = f's3://{base_aws_bucket}/vfl_predict/active_predictions.csv'\n",
    "vfl_predict_passive_storage_path = f's3://{base_aws_bucket}/vfl_predict/passive_predictions.csv'\n",
    "\n",
    "base_aws_bucket #Prints the base_aws_bucket name for reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the taskbuilder \n",
    "\n",
    "Specify the task runner name (e.g. `mytaskrunner`) as `task_runner_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import integrate_ai_sdk\n",
    "from integrate_ai_sdk.taskgroup.taskbuilder.integrate_ai import IntegrateAiTaskBuilder\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup\n",
    "from integrate_ai_sdk.taskgroup.taskbuilder import taskrunner_context\n",
    "from integrate_ai_sdk.taskgroup.taskbuilder.integrate_ai import IntegrateAiTaskBuilder\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup\n",
    "\n",
    "iai_tb_aws = IntegrateAiTaskBuilder(client=client,\n",
    "   task_runner_id=aws_taskrunner_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Perform EDA in Individual Mode\n",
    "\n",
    "This example task demonstrates how to run an exploratory data analysis (EDA) session in Individual mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset configuration\n",
    "\n",
    "dataset_config = {\"dataset_one\": [], \"dataset_two\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1e4a4add14'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and start the EDA session \n",
    "\n",
    "eda_session = client.create_eda_session(\n",
    "    name=\"Testing notebook - EDA\",\n",
    "    description=\"I am testing EDA session creation with a task runner through a notebook\",\n",
    "    data_config=dataset_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "eda_session.id  #Prints the EDA session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "eda_task_group_context = (\n",
    "        SessionTaskGroup(eda_session) \\\n",
    "        .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "        .add_task(iai_tb_aws.eda(dataset_name=\"dataset_one\", dataset_path=active_train_path))\\\n",
    "        .add_task(iai_tb_aws.eda(dataset_name=\"dataset_two\", dataset_path=passive_train_path))\\\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the task group status\n",
    "\n",
    "import json\n",
    "\n",
    "for i in eda_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "eda_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: EDA session complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view eda session results"
    ]
   },
   "outputs": [],
   "source": [
    "# Session complete, retrieve the results \n",
    "\n",
    "results = eda_session.results()[\"dataset_one\", \"dataset_two\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get shape of results"
    ]
   },
   "outputs": [],
   "source": [
    "results.mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get info from col"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_one = eda_session.results()[\"dataset_one\"]\n",
    "dataset_one_count = dataset_one[\"x0\"].count()\n",
    "dataset_one[\"x0\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Create an HFL FFNet Training Session\n",
    "\n",
    "The documentation for [creating a session](https://documentation.integrateai.net/#tutorial-ffnet-model-training-with-a-sample-local-dataset-iai_ffnet) gives additional context into the parameters that are used during training session creation.<br />\n",
    "For this session we are going to be using two training clients and two rounds. \n",
    "\n",
    "You can find the model config and data schema details in the [integrate.ai end user tutorial](https://documentation.integrateai.net/#understanding-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assign schemas"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config = {\n",
    "    \"experiment_name\": \"test_synthetic_tabular\",\n",
    "    \"experiment_description\": \"test_synthetic_tabular\",\n",
    "    \"strategy\": {\"name\": \"FedAvg\", \"params\": {}},\n",
    "    \"model\": {\"params\": {\"input_size\": 15, \"hidden_layer_sizes\": [6, 6, 6], \"output_size\": 2}},\n",
    "    \"balance_train_datasets\": False,\n",
    "    \"ml_task\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"params\": {\n",
    "            \"loss_weights\": None,\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.2, \"momentum\": 0.0}},\n",
    "    \"differential_privacy_params\": {\"epsilon\": 4, \"max_grad_norm\": 7},\n",
    "    \"save_best_model\": {\n",
    "        \"metric\": \"loss\",  # to disable this and save model from the last round, set to None\n",
    "        \"mode\": \"min\",\n",
    "    },\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_schema = {\n",
    "    \"predictors\": [\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\", \"x11\", \"x12\", \"x13\", \"x14\"],\n",
    "    \"target\": \"y\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start the training session\n",
    "\n",
    "training_session = client.create_fl_session(\n",
    "    name=\"Testing notebook\",\n",
    "    description=\"I am testing session creation with a task runner through a notebook\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_ffnet\",\n",
    "    model_config=model_config,\n",
    "    data_config=data_schema,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "training_session.id # Prints the training session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start batch clients"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "task_group = (\n",
    "    SessionTaskGroup(training_session)\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path1, test_path=test_path1, use_gpu=False))\\\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path2, test_path=test_path2, use_gpu=False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_group_context = task_group.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the submitted tasks\n",
    "\n",
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: HFL Session Complete!\n",
    "Now you can view the training metrics and start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# Retrieve the session metrics\n",
    "\n",
    "training_session.metrics().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the session metrics\n",
    "\n",
    "fig = training_session.metrics().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Create a PRL Session for linking two or more datasets\n",
    "\n",
    "To create a PRL session, specify a `dataset_config` dictionary indicating the client names and columns to use as identifiers to link the datasets to each other. The number of expected clients will be inferred as the number of items in dataset_config (i.e., two). These client names are referenced for the compute on the PRL session and for any sessions that use the PRL session downstream.\n",
    "\n",
    "For this session, two clients are going to be providing data. Client 1 and client 2 are naming their clients client_1 and client_2 respectively. Their datasets will be linked by the \"id\" column in any provided datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create prl data config"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify PRL dataset configuration \n",
    "\n",
    "prl_data_config = {\n",
    "    \"clients\": {\n",
    "        \"active_client\": {\"id_columns\": [\"id\"]},\n",
    "        \"passive_client\": {\"id_columns\": [\"id\"]},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create prl session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start PRL session\n",
    "\n",
    "prl_session = client.create_prl_session(\n",
    "    name=\"Testing notebook - PRL\",\n",
    "    description=\"I am testing PRL session creation with a task runner through a notebook\",\n",
    "    data_config=prl_data_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "prl_session.id #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start prl training batch clients"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "task_group = (SessionTaskGroup(prl_session)\\\n",
    ".add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    ".add_task(iai_tb_aws.prl(train_path=active_train_path, test_path=active_test_path, client_name=\"active_client\"))\\\n",
    ".add_task(iai_tb_aws.prl(train_path=passive_train_path, test_path=passive_test_path, client_name=\"passive_client\"))\n",
    ")\n",
    "\n",
    "task_group_context = task_group.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the status of the task group\n",
    "\n",
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: PRL Session Complete!\n",
    "Now you can view the overlap stats for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view prl session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# View PRL session metrics\n",
    "\n",
    "metrics = prl_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a: Create a VFL Training Session using the PRL session from Task 3\n",
    "\n",
    "To create a VFL train session, specify the `prl_session_id` indicating the session you just ran to link the datasets together. The `vfl_mode` needs to be set to `train`.\n",
    "\n",
    "For more information about vertical federated learning with a SplitNN model strategy, see [VFL SplitNN Model Trianing](https://documentation.integrateai.net/#vfl-splitnn-model-training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "set vfl data config"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config = {\n",
    "    \"strategy\": {\"name\": \"SplitNN\", \"params\": {}},\n",
    "    \"model\": {\n",
    "        \"feature_models\": {\n",
    "            \"passive_client\": {\"params\": {\"input_size\": 7, \"hidden_layer_sizes\": [6], \"output_size\": 5}},\n",
    "            \"active_client\": {\"params\": {\"input_size\": 8, \"hidden_layer_sizes\": [6], \"output_size\": 5}},\n",
    "        },\n",
    "        \"label_model\": {\"params\": {\"hidden_layer_sizes\": [5], \"output_size\": 2}},\n",
    "    },\n",
    "    \"ml_task\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"params\": {\n",
    "            \"loss_weights\": None,\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.2, \"momentum\": 0.0}},\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_config = {\n",
    "        \"passive_client\": {\n",
    "            \"label_client\": False,\n",
    "            \"predictors\": [\"x1\", \"x3\", \"x5\", \"x7\", \"x9\", \"x11\", \"x13\"],\n",
    "            \"target\": None,\n",
    "        },\n",
    "        \"active_client\": {\n",
    "            \"label_client\": True,\n",
    "            \"predictors\": [\"x0\", \"x2\", \"x4\", \"x6\", \"x8\", \"x10\", \"x12\", \"x14\"],\n",
    "            \"target\": \"y\",\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create training session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a VFL training session\n",
    "\n",
    "vfl_train_session = client.create_vfl_session(\n",
    "    name=\"Testing notebook - VFL Train\",\n",
    "    description=\"I am testing VFL Train session creation with a task runner through a notebook\",\n",
    "    prl_session_id=prl_session.id,\n",
    "    vfl_mode='train',\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_ffnet\",\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "vfl_train_session.id    #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start training task"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "vfl_task_group_context = (SessionTaskGroup(vfl_train_session)\\\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.vfl_train(train_path=active_train_path, \n",
    "                                    test_path=active_test_path, \n",
    "                                    batch_size=1024,  \n",
    "                                    client_name=\"active_client\", \n",
    "                                    storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.vfl_train(train_path=passive_train_path, \n",
    "                                    test_path=passive_test_path, \n",
    "                                    batch_size=1024, \n",
    "                                    client_name=\"passive_client\", \n",
    "                                    storage_path=aws_storage_path))\\\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "check training task session id"
    ]
   },
   "outputs": [],
   "source": [
    "# Check the status of the tasks\n",
    "\n",
    "for i in vfl_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "vfl_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "vfl_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: VFL Session Complete!\n",
    "Now you can view the VFL training metrics and start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view training metrics"
    ]
   },
   "outputs": [],
   "source": [
    "metrics = vfl_train_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = vfl_train_session.metrics().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b: Make a Prediction on the trained VFL model\n",
    "\n",
    "To create a VFL predict session, specify the `prl_session_id` indicating the session you ran to link the datasets together. You also need the `training_id` of the VFL train session that was run using the same `prl_session_id`. \n",
    "\n",
    "The `vfl_mode` must be set to `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create predict session"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a VFL predict session\n",
    "\n",
    "vfl_predict_session = client.create_vfl_session(\n",
    "    name=\"Testing notebook - VFL Predict\",\n",
    "    description=\"I am testing VFL Predict session creation with an AWS task runner through a notebook\",\n",
    "    prl_session_id=prl_session.id,\n",
    "    training_session_id=vfl_train_session.id,\n",
    "    vfl_mode=\"predict\",\n",
    "    data_config=data_config,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "vfl_predict_session.id  # Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start prediction task"
    ]
   },
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "vfl_predict_task_group_context = (SessionTaskGroup(vfl_predict_session)\\\n",
    ".add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    ".add_task(iai_tb_aws.vfl_predict(\n",
    "        client_name=\"active_client\", \n",
    "        dataset_path=active_test_path, \n",
    "        raw_output=True,\n",
    "        batch_size=1024, \n",
    "        storage_path=vfl_predict_active_storage_path))\\\n",
    ".add_task(iai_tb_aws.vfl_predict(\n",
    "        client_name=\"passive_client\",\n",
    "        dataset_path=passive_test_path,\n",
    "        batch_size=1024,\n",
    "        raw_output=True,\n",
    "        storage_path=vfl_predict_passive_storage_path))\\\n",
    ".start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the tasks\n",
    "\n",
    "for i in vfl_predict_task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "\n",
    "vfl_predict_task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wait for the tasks to complete (success = True)\n",
    "\n",
    "vfl_predict_task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: VFL Predict Session Complete!\n",
    "\n",
    "Now you can view the VFL predictions and evaluate the performance as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view predict session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "# Retrieve the metrics\n",
    "\n",
    "metrics = vfl_predict_session.metrics().as_dict()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if TEST_MODE in ['aws_azure', 'aws']:\n",
    "    print(vfl_predict_active_storage_path)\n",
    "    df_pred = pd.read_csv(vfl_predict_active_storage_path)\n",
    "else:\n",
    "    print(vfl_predict_active_storage_path_az)\n",
    "    with smart_open.open(vfl_predict_active_storage_path_az, \n",
    "                         \"r\", \n",
    "                         transport_params=_smart_open_transport_params(vfl_predict_active_storage_path_az)) as f:\n",
    "        df_pred = pd.read_csv(f)\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Using a custom model in an HFL session\n",
    "\n",
    "Choose a name for your custom model, and set the path for the model and data configurations. For more information about creating a custom model, see \n",
    "\n",
    "Note that the name for your custom model must be unique. This means that the name for your custom model cannot already be in the Package Name column of the Custom Models Packages Table in the Model Library Page of the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(package_path)\n",
    "print(package_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(model_config_path, \"r\") as f:\n",
    "    lstm_model_config = json.load(f)\n",
    "\n",
    "with open(data_config_path, \"r\") as f:\n",
    "    data_schema = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload customized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_model(\n",
    "    package_path=package_path,\n",
    "    dataset_path=local_dataset_path,\n",
    "    package_name=package_name,\n",
    "    sample_model_config_path=model_config_path,\n",
    "    sample_data_config_path=data_config_path,\n",
    "    batch_size=256,\n",
    "    task=\"classification\",\n",
    "    test_only=False,\n",
    "    description=\"A custom LSTM model.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an HFL session with the custom package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model configuration\n",
    "\n",
    "model_config = {\n",
    "    \"strategy\": {\"name\": \"FedAvg\", \"params\": {}},\n",
    "    \"model\": {\"params\": lstm_model_config},\n",
    "    \"ml_task\": {\"type\": \"classification\", \"params\": {}},\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.9, \"momentum\": 0.9}},\n",
    "    \"differential_privacy_params\": {\"epsilon\": 4, \"max_grad_norm\": 7},\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start the training session\n",
    "\n",
    "training_session = client.create_fl_session(\n",
    "    name=\"LSTM custom model notebook\",\n",
    "    description=\"Training a custom LSTM model using a jupyter notebook.\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=5,\n",
    "    package_name=package_name,\n",
    "    model_config=model_config,\n",
    "    data_config=data_schema,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "training_session.id     #Prints the session ID for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a task group with one task for the server, and one for each of the clients joining the session\n",
    "\n",
    "task_group_context = (\n",
    "    SessionTaskGroup(training_session)\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\n",
    "    .add_task(iai_tb_aws.hfl(train_path=aws_dataset_path, test_path=aws_dataset_path, approve_custom_package=True))\n",
    "    .add_task(iai_tb_aws.hfl(train_path=aws_dataset_path, test_path=aws_dataset_path, approve_custom_package=True))\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_group_context.wait(60*5, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Custom model session complete!\n",
    "\n",
    "Now you can retrieve the metrics for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_session.metrics().as_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Create a linear inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model and data configurations\n",
    "\n",
    "model_config_logit = {\n",
    "    \"strategy\": {\"name\": \"LogitRegInference\", \"params\": {}},\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_config_logit = {\n",
    "    \"target\": \"y\",\n",
    "    \"shared_predictors\": [\"x1\", \"x2\"],\n",
    "    \"chunked_predictors\": [\"x0\", \"x3\", \"x10\", \"x11\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start a linear inference session \n",
    "\n",
    "training_session_logit = client.create_fl_session(\n",
    "    name=\"Testing linear inference session\",\n",
    "    description=\"I am testing linear inference session creation using a task runner through a notebook\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=5,\n",
    "    package_name=\"iai_linear_inference\",\n",
    "    model_config=model_config_logit,\n",
    "    data_config=data_config_logit,\n",
    "    startup_mode=\"external\"\n",
    ").start()\n",
    "\n",
    "training_session_logit.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a task group\n",
    "\n",
    "task_group_context = (\n",
    "    SessionTaskGroup(training_session_logit)\n",
    "    .add_task(iai_tb_aws.fls(storage_path=aws_storage_path))\\\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path1, test_path=test_path1))\n",
    "    .add_task(iai_tb_aws.hfl(train_path=train_path2_aws, test_path=test_path2_aws)).start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in task_group_context.contexts.values():\n",
    "    print(json.dumps(i.status(), indent=4))\n",
    "task_group_context.monitor_task_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_group_context.wait(60*5, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f8223dd0c77fd300f5027a07540ab82ef7f159d3d8a00663aa8a0c2ca691ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
